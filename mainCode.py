# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LoNv_elQ2Ky58ggUQenVURzb92QoPC4T

# Installing Dectectron2 + packages
"""

# !python -m pip install pyyaml==5.1
import sys, os, distutils.core
# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).
# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions
# git clone 'https://github.com/facebookresearch/detectron2'
# dist = distutils.core.run_setup("./detectron2/setup.py")
# python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}
# sys.path.insert(0, os.path.abspath('./detectron2'))

# Properly install detectron2. (Please do not install twice in both ways)
#python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
#from google.colab.patches import cv2_imshow
from PIL import Image
from detectron2.structures import PolygonMasks, Instances, BoxMode
from pycocotools import mask as mask_utils

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

import torch, detectron2
#!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

"""# Getting the Dataset and Making it the Correct Format"""

# download, decompress the data
# !wget

folder_path = "/Users/khadeejaabbas/Downloads/PhenoBench"
# # Mount Google Drive
# drive.mount('/content/drive')

# # Change directory to your folder containing the zip file
# os.chdir('/content/drive/MyDrive')

# unzip PhenoBench.zip


def process_image_files(folder_path):
    if not os.path.exists(folder_path):
        print(f"Folder '{folder_path}' does not exist.")
        return

    for root, dirs, files in os.walk(folder_path):
        for file in files:
            # Process each image file
            if file.endswith(".jpg") or file.endswith(".png"):
                image_path = os.path.join(root, file)
                print("Processing:", image_path)
                # Your image processing code goes here

def trying(img_dir):
  try:
      contents = os.listdir(img_dir)
      # Print the list of files and directories
      print(contents)
  except FileNotFoundError:
      print(f"The specified path '{img_dir}' does not exist.")
  except PermissionError:
      print(f"Permission denied for the path '{img_dir}'. Check your read permissions.")
  except Exception as e:
      print(f"An error occurred: {e}")

def reading_inside_of_folders(img_dir):
  try:
      # List all folders in the specified path
      #folders = [f for f in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, f))]
      folders = [f for f in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, f)) and f != ".DS_Store"]

      # Access the contents within each folder
      for folder in folders:
          folder_path = os.path.join(img_dir, folder)
          contents = os.listdir(folder_path)
          #print(f"Contents of {folder}: {contents}")
  except FileNotFoundError:
      print(f"The specified path '{img_dir}' does not exist.")
  except PermissionError:
      print(f"Permission denied for the path '{img_dir}'. Check your read permissions.")
  except Exception as e:
      print(f"An error occurred: {e}")

# Example function that returns either a NumPy array or None
def get_data(datanew):
    # Some code to fetch data
    data = np.array(datanew)
    return data

def binary_mask_to_polygon(binary_mask):
    """Convert binary mask to COCO-style polygon format."""
    # im = cv2.imread(binary_mask)
    # cv2_imshow(im)
    # image = Image.open(binary_mask)

    # Usage
    result = get_data(binary_mask)
    print(result)

    if result is not None:
        # Only try to use the data if it's not None
        modified_result = result.astype(np.uint8)
        print(modified_result)
    else:
        print("Data is None. Cannot proceed.")
    contours, _ = cv2.findContours(binary_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    polygons = [c.flatten().tolist() for c in contours]
    return polygons

# # Assuming you have the binary mask as 'binary_mask'
# # Convert binary mask to polygon format
# polygons = binary_mask_to_polygon(binary_mask)

# Create a list of dictionaries with COCO-style annotation format
# annotations = []
# for polygon in polygons:
#     annotation = {
#         "segmentation": [polygon],
#         "category_id": 0,  # Replace with the category ID for the instance
#     }
#     annotations.append(annotation)

# 'annotations' is now in the COCO-style format

def image_masks_to_coco_format(image_masks):
    """
    Convert image masks to instance segmentation masks in COCO format.

    Parameters:
        image_masks (list): List of binary masks as numpy arrays.

    Returns:
        Instances: An Instances object containing instance segmentation masks in COCO format.
    """
    print(image_masks)
    result = get_data(image_masks)
    print(result)

    if result is not None:
        # Only try to use the data if it's not None
        modified_result = result.astype(np.uint8)
        print(modified_result)
    else:
        print("Data is None. Cannot proceed.")
    im = cv2.imread(image_masks)
    # cv2_imshow(im)
    # new = binary_mask_to_polygon(im)


    # Convert binary masks to RLE (Run-Length Encoding) format
    rles = mask_utils.encode(np.asfortranarray(im))

    # Create a PolygonMasks object from the RLE-encoded masks
    polygon_masks = PolygonMasks(rles)

    # Create an empty Instances object to store the instance segmentation masks
    instances = Instances((256, 256))  # Replace with the image size or use None for variable-sized images

    # Set the "pred_masks" field of the Instances object with the polygon masks
    instances.pred_masks = polygon_masks

    # For COCO format, set the number of instances equal to the number of masks
    instances.set("gt_classes", np.ones(len(im), dtype=int))

    return instances

# Example usage:
# Assuming you have a list of binary masks as 'image_masks'
# Convert image masks to COCO format

# 'instances' is now an Instances object containing the instance segmentation masks in COCO format

def get_plant_dicts(img_dir):
    # List all files and directories in the specified path
    reading_inside_of_folders(img_dir)
    # contents = os.listdir(img_dir)
    # improv_contents = [file for file in contents if file != ".DS_Store"]
    # print(improv_contents)
    # holder = []
    # instances = []
    # Loop through the contents and print each entry

    # for i in improv_contents:
    #   # Replace "images_folder_path" with the actual path to your "images" folder
    #   images_folder_path = i
    #   process_image_files(images_folder_path)

      # with open(i, 'r') as file:
      #   content_of_file = file.read()
      #   instances.append(image_masks_to_coco_format(content_of_file))
      # instances.append(i)

      # holder.append(entry)
      #instances.append(image_masks_to_coco_format(entry))


    # for i in holder:
    #   instances.append(image_masks_to_coco_format(i))

    try:
      # List all folders in the specified path
      #folders = [f for f in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, f))]
        folders = [f for f in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, f)) and f != ".DS_Store"]

      # Access the contents within each folder

     # Use os.walk() to traverse through the directory structure
        for folder in folders:
            folder_path = os.path.join(img_dir, folder)
            for root, _, files in os.walk(folder_path):
                for file in files:
        # for i in folders:
        #     print(i)
        #     for root, _, files in os.walk(i):
        #         print("bye")
        #         for file in files:
                    print("r u working?")
                    print(file)
                    file_path = os.path.join(root, file)
        

                    # Check if the file is an image (you can add more image extensions as needed)
                    # if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    #     # Process the image using its file_path
                    #     print(f"Processing image: {file_path}")
            # for folder in folders:
            #     print(folder)
            #     folder_path = os.path.join(img_dir, folder)
            #     contents = os.listdir(folder_path)
                #adding to this code
                    with open(file_path) as f:
                        imgs_anns = json.load(f)
                    # print(f"Contents of {file}: {contents}")
                    dataset_dicts = []
                    for idx, v in enumerate(imgs_anns.values()):
                        record = {}

                    filename = os.path.join(img_dir, v["filename"])
                    height, width = cv2.imread(filename).shape[:2]

                    record["file_name"] = filename
                    record["image_id"] = idx
                    record["height"] = height
                    record["width"] = width

                    annos = v["regions"]
                    objs = []
                    for _, anno in annos.items():
                        assert not anno["region_attributes"]
                        anno = anno["shape_attributes"]
                        px = anno["all_points_x"]
                        py = anno["all_points_y"]
                        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
                        poly = [p for x in poly for p in x]

                        obj = {
                            "bbox": [np.min(px), np.min(py), np.max(px), np.max(py)],
                            "bbox_mode": BoxMode.XYXY_ABS,
                            "segmentation": [poly],
                            "category_id": 0,
                        }
                        objs.append(obj)
                    record["annotations"] = objs
                    dataset_dicts.append(record)
                    return dataset_dicts
    except FileNotFoundError:
        print(f"The specified path '{img_dir}' does not exist.")
        exit()

    except PermissionError:
        print(f"Permission denied for the path '{img_dir}'. Check your read permissions.")
        exit()

    except Exception as e:
        print(f"An error occurred: {e}")
        exit()

# ORIGINAL DOWN BELOW
    # json_file = os.path.join(img_dir, improv_contents)
    # json_file = os.path.join(img_dir, "via_region_data.json")
    # with open(json_file) as f:
    #     imgs_anns = json.load(f)
    '''
    dataset_dicts = []
    for idx, v in enumerate(imgs_anns.values()):
        record = {}

        filename = os.path.join(img_dir, v["filename"])
        height, width = cv2.imread(filename).shape[:2]

        record["file_name"] = filename
        record["image_id"] = idx
        record["height"] = height
        record["width"] = width

        annos = v["regions"]
        objs = []
        for _, anno in annos.items():
            assert not anno["region_attributes"]
            anno = anno["shape_attributes"]
            px = anno["all_points_x"]
            py = anno["all_points_y"]
            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
            poly = [p for x in poly for p in x]

            obj = {
                "bbox": [np.min(px), np.min(py), np.max(px), np.max(py)],
                "bbox_mode": BoxMode.XYXY_ABS,
                "segmentation": [poly],
                "category_id": 0,
            }
            objs.append(obj)
        record["annotations"] = objs
        dataset_dicts.append(record)
    return dataset_dicts
    '''
# for d in ["train", "val"]:
#     DatasetCatalog.register("plant_" + d, lambda d=d: get_plant_dicts("plant/" + d))
#     MetadataCatalog.get("plant_" + d).set(thing_classes=["plant"])
plant_metadata = MetadataCatalog.get("plant_train")


specfic_folder_path=("/Users/khadeejaabbas/Downloads/PhenoBench/train")
dataset_dicts = get_plant_dicts(specfic_folder_path)

# for d in random.sample(dataset_dicts, 3):
#     img = cv2.imread(d["file_name"])
#     visualizer = Visualizer(img[:, :, ::-1], metadata=plant_metadata, scale=0.5)
#     out = visualizer.draw_dataset_dict(d)
#     cv2_imshow(out.get_image()[:, :, ::-1])
